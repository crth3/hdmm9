{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Templates & Links for the Course M9\n",
    "This notebook contains useful templates (e.g. shell commands, code snippets) as well as links for the course M9 (BI and Big Data Architectures).\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links to the Used Web Interfaces\n",
    "- JupyterHub (for Python development with Jupyter Notebooks): see slides\n",
    " - user: see slides\n",
    " - password: provided in lecture\n",
    "- Hue (the Hadoop web interface): see slides\n",
    " - user: see slides\n",
    " - password: provided in lecture\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2 - Big Data and Hadoop Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the SSH connection to Cloudera\n",
    "In order to connect to our hadoop on-premise system (cloudera) use an SSH-client and connect to \n",
    "- Host: see slides\n",
    "- Port: 22\n",
    "- User: see slides\n",
    "- Password: provided in lecture\n",
    "\n",
    "Windows: download Putty or MobaXterm (save the connection for later usage)<br>\n",
    "Mac: open a terminal and execute <code>ssh ...</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the local PostgreSQL database\n",
    "<code>mysql -u xxx -p -D xxx</code>\n",
    "<br>\n",
    "Password: provided in lecture<br>\n",
    "Within the client:<br>\n",
    "<code>show databases;</code><br>\n",
    "<code>show tables;</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS NameNode and DataNode\n",
    "- http://xxx:50070\n",
    "- http://xxx:50075"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2.1: HDFS Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -ls /user/cloudera/\n",
    "hadoop fs -ls /user/cloudera/2_mr\n",
    "hadoop fs -cat /user/cloudera/2_mr/orders.csv\n",
    "cat 123456 > test_matrnr.txt\n",
    "hadoop fs -mkdir /user/cloudera/students/.../test\n",
    "hadoop fs -put test_matrnr.txt /user/cloudera/students/.../test\n",
    "hadoop fs -ls /user/cloudera/students/.../\n",
    "hadoop fs -cat /user/cloudera/students/.../test/test_martnr.txt\n",
    "hadoop fs -rm /user/cloudera/students/.../test/*\n",
    "hadoop fs -rmdir /user/cloudera/students/.../test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2.2: MapReduce Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/cloudera/2_mr\n",
    "hadoop jar wordcount.jar WordCount /user/cloudera/students/.../input/ /user/cloudera/students/.../output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCP Registration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Lecture 3 - Batch Data Ingestion and Data Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3.1: Manual file upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>hadoop fs -put sample-orders.csv /user/cloudera/student/.../3_batch_ingest/landing</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow:<br>\n",
    "- Source: <code>/user/cloudera/3_batch_ingest/landing/sample-orders.csv</code>\n",
    "- Target: <code>/user/cloudera/3_batch_ingest/order_history/sample-orders_${wf:id()}.csv</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to Oozie:\n",
    "- Workflows: http://xxx:8888/oozie/editor/workflow/list/\n",
    "- Coordinators: http://xxx:8888/oozie/list_oozie_coordinators/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3.2: Structured Data Upload with sqoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>sqoop import --table categories --connect jdbc:mysql://localhost:3306/retail_db --username=xxx --password=xxx --target-dir /user/cloudera/students/.../3_batch_ingest/sqoop_categories --delete-target-dir</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3.3: sqoop Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>sqoop import --table order_items --connect jdbc:mysql://localhost:3306/retail_db --username=xxx --password=xxx --target-dir /user/cloudera/3_batch_ingest/sqoop_orderitems_parquet --delete-target-dir --as-parquetfile</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3.4: Hive Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>beeline\n",
    "!connect jdbc:hive2://localhost:10000 xxx\n",
    "show databases;\n",
    "showw tables;\n",
    "describe tablename;</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive Data Model - Partitioning\n",
    "\n",
    "<code>CREATE_TABLE Sales (sale_id INT, customer_id INT, amount FLOAT)\n",
    "PARTITIONED BY (country STRING, year INT, month INT)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impala\n",
    "<code>create table student (id int, number int);</code>\n",
    "\n",
    "<code>insert into student values (1,10), (2, 20)</code>\n",
    "\n",
    "Hive vs. Impala Comparison:\n",
    "https://www.dezyre.com/article/impala-vs-hive-difference-between-sql-on-hadoop-components/180\n",
    "\n",
    "Hive Partitioning & Clustering Performance: \n",
    "https://blog.cloudera.com/blog/2017/12/faster-performance-for-selective-queries/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3.5: Manual Data Ingestion\n",
    "<code>beeline\n",
    "!connect jdbc:hive2://localhost:10000 xxx</code>\n",
    "\n",
    "- Generation of table: <code>CREATE TABLE my_table(product STRING, inventory INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' STORED AS TEXTFILE;</code>\n",
    "- Insert data manually: <code>INSERT INTO my_table VALUES ('A', 10), ('B', 20);</code>\n",
    "- Insert data from a file: <code>LOAD DATA INPATH '/user/cloudera/3_batch_ingest/manual_ingestion/inventory.txt' INTO TABLE my_table;</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3.6: Hive with Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>sqoop import --table order_items --connect jdbc:mysql://localhost:3306/retail_db --username=xxx --password=xxx --warehouse-dir=/user/hive/warehouse --hive-import --as-parquetfile --hive-overwrite</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3.7: Hive with Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dynamic partitioning: <code>set hive.exec.dynamic.partition.mode=nonstrict</code>\n",
    "- Insert data into partitioned table: <code>insert overwrite table order_items_partitioned partition(order_item_product_id) select order_item_id, order_item_order_id, order_item_product_price, order_item_quantity, order_item_subtotal, order_item_product_id from order_items</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3.8:  SF Police Incidents use case\n",
    "### Preparation\n",
    "\n",
    "- Add jar-file for SerDe: <code>ADD JAR /usr/lib/hive/lib/opencsv-2.3.jar;</code>\n",
    "- Create table: <code>CREATE TABLE sf_incidents (IncidntNum int, Category string, Descript string, DayOfWeek string, dDate date, Ttime string, PdDistrict string, Resolution string, Address string, x DECIMAL(9, 6), y DECIMAL(9, 6), LLocation string, PdId string) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES (\"separatorChar\" = \"\\\\,\", \"quoteChar\" = \"\\\\\"\", \"escapeChar\" = \"\\\\\\\\\") STORED AS TEXTFILE;</code>\n",
    "- Load data: <code>LOAD DATA INPATH '/user/cloudera/3_batch_ingest/sf_crime/Police_Department_Incidents.csv' INTO TABLE sf_incidents;</code>\n",
    "\n",
    "### Queries\n",
    "\n",
    "- <code>SELECT Category, count(Category) from sf_incidents group by Category;</code>\n",
    "- <code>SELECT Category, DayOfWeek, count(Category) from sf_incidents where category= \"LIQUOR LAWS\" group by category, DayOfWeek;</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3.9: Import all order tables with sqoop\n",
    "\n",
    "Drop tables we created so far:\n",
    "<code>!connect jdbc:hive2://localhost:10000 xxx\n",
    "drop table if exists order_items;\n",
    "drop table if exists categories;\n",
    "drop table if exists customers;</code>\n",
    "\n",
    "Import whole dataset from MySQL:\n",
    "<code>sqoop import-all-tables -m 1 --connect jdbc:mysql://quickstart:3306/retail_db --username=xxx --password=xxx --compression-codec=snappy --as-parquetfile --warehouse-dir=/user/hive/warehouse --hive-import</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Lecture 4 - Batch Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Example (Slide 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add  # Spark's implementation of adding two numbers\n",
    "\n",
    "# our inpurt RDD\n",
    "rdd = sc.parallelize([('NY',10),('NY',20),('NJ',15),('NY',19), ('NJ', 10)])\n",
    "\n",
    "# build the sum per city\n",
    "rdd.reduceByKey(add).collect()\n",
    "\n",
    "\n",
    "# make it a bit more complex: first map city names, then build the sum\n",
    "def convert(kv):\n",
    "    mapping = {'NY':'New York', 'NJ':'New Jersey'}\n",
    "    return (mapping[kv[0]], kv[1])\n",
    "\n",
    "rdd.map(convert).reduceByKey(add).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another example\n",
    "df = sqlContext.read.load(\"gs://xxx/orders.csv\", format=\"com.databricks.spark.csv\", header=\"true\", inferSchema=\"true\")\n",
    "\n",
    "# get the first five rows\n",
    "df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 4.1 - Spark & Scala on the Cloudera Machine\n",
    "<code>// create an RDD from an HDFS file\n",
    "val file = sc.textFile(\"hdfs://localhost/user/cloudera/4_batch_process/1_input_wordcount/*\")\n",
    "// transform the RDD: one line for each word \n",
    "val words = file.flatMap(line => line.split(\" \"))\n",
    "// execute action: count all words (evaluation starts afterwards)\n",
    "val number_of_words = words.count()\n",
    "// transform the RDD: (1) prepare for wordcount (2) reduce by key and sum occurences\n",
    "val occurences_per_word = words.map(word => (word, 1)).reduceByKey(_ + _)\n",
    "// show the occurences (these are both actions)\n",
    "occurences_per_word.collect()\n",
    "occurences_per_word.saveAsTextFile(\"hdfs://localhost/user/cloudera/4_batch_process/output_wordcount\")</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 4.2 - Spark & Scala on a DataProc Cluster\n",
    "### Using the Web SSH-tool\n",
    "<code>// prepare for Spark Dataset usage\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().appName(\"Spark Test HDM M9\").getOrCreate()\n",
    "import spark.implicits._\n",
    "\n",
    "// read the csv-File with the order items\n",
    "val df = spark.read.option(\"header\", \"true\") .option(\"inferSchema\", \"true\").csv(\"gs://spark_scala/order_items.csv\")\n",
    "// look at the metadata\n",
    "df.printSchema() \n",
    "df.head() // look at a sample (first row)</code>\n",
    "<code>// show the order_id column\n",
    "df.select(\"order_id\").show() \n",
    "// show two columns\n",
    "df.select(\"order_item_id\", \"order_id\").show()\n",
    "// group by the order_id and count the number of line items\n",
    "df.groupBy(\"order_id\").count().show()\n",
    "// show aggregates per product\n",
    "df.groupBy(\"product_id\").sum(\"quantity\", \"subtotal\").show()\n",
    "// Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"order_items\")\n",
    "val sqlDF = spark.sql(\"SELECT * FROM order_items WHERE product_id=5\")\n",
    "sqlDF.show()\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 4.3 - GCP with PySpark and Jupyter Notebooks (Alternative 1: local SSH tunnel)\n",
    "This is the preffered method, also for the project.<br>Recommended WiFi: eduroam<br><br>\n",
    "Video showing the process for Mac: https://www.youtube.com/watch?v=EdNWgZ4cOWo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: DataProc setup - Initialization Action for JupyterHub\n",
    "Create a cluster with the following initialization action (within advanced settings):\n",
    "\n",
    "<code>gs://dataproc-initialization-actions/jupyter/jupyter.sh</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Establish SSH connection to the master node\n",
    "\n",
    "We now want to establish a secure connection to our master node via SSH. We will not use this connection to prompt commands (like in the web shell we used), but only use it as a \"tunnel\" to reach the cluster network securely and especially within this network the master node with Jupyter running.\n",
    "\n",
    "<font color=\"red\"><b>Open a terminal on your Laptop (shell/Eingabeaufforderung/...)!</b></font>\n",
    "\n",
    "Initialize your gcloud SDK:<br>\n",
    "<code>gcloud init</code>\n",
    "<br><br>\n",
    "Set your default project:<br>\n",
    "<code>gcloud config set project project_id</code>\n",
    "<br><i>Note: project_id is the project ID - needs to be replaced with your project</i>\n",
    "<br><br>\n",
    "Create an ssh connection to the master node:<br>\n",
    "<code>gcloud compute ssh --zone=europe-west1-b --ssh-flag=\"-D\" --ssh-flag=\"10000\" --ssh-flag=\"-N\" \"cluster_name-m\"</code>\n",
    "<br><i>Note: cluster_name is the name of your DataProc cluster</i>\n",
    "<br><br>\n",
    "<font color=\"red\"><b>Do not close the terminal (and on Windows also not the Putty window)!</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Open a browser with SSH-Tunnel as proxy and within this Jupyter\n",
    "\n",
    "<font color=\"red\"><b>Now open a second terminal/shell on your laptop!</b></font>\n",
    "<br>We now want to open a browser that uses the SSH connection as a proxy (i.e. \"tunnels\" all traffic through this connection such that we can reach Jupyter running on our master-node.\n",
    "\n",
    "#### Generic command\n",
    "<code>\"browser executable path\" \"http://cluster_name-m:8123\" --proxy-server=\"socks5://localhost:10000\" --host-resolver-rules=\"MAP * 0.0.0.0 , EXCLUDE localhost\" --user-data-dir=C:\\temp</code>\n",
    "\n",
    "#### On a Windows host with chrome and pyspark as DataProc cluster name\n",
    "\n",
    "https://cloud.google.com/dataproc/docs/tutorials/jupyter-notebook\n",
    "\n",
    "<code>\"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\" \"http://cluster_name-m:8123\" --proxy-server=\"socks5://localhost:10000\" --host-resolver-rules=\"MAP * 0.0.0.0 , EXCLUDE localhost\" --user-data-dir=c:\\Temp</code>\n",
    "\n",
    "#### On a Mac host with chrome\n",
    "<code>/Applications/Google\\\\ Chrome.app/Contents/MacOS/Google\\\\ Chrome --proxy-server=\"socks5://localhost:10000\" --user-data-dir=/tmp/${HOSTNAME}</code>\n",
    "\n",
    "\n",
    "### Step 4: Open Jupyter (on Windows automatically openend)\n",
    "URL in opened browser window: http://cluster_name-m:8123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 4.3 - GCP with PySpark and Jupyter Notebooks (Alternative 2: cloud shell)\n",
    "### Step 1: Create a cluster\n",
    "See above \n",
    "### Step 2: Open the cloud shell\n",
    "Enter this command to establish an SSH connection from cloud shell to your master node:<br>\n",
    "<code>gcloud compute ssh cluster_name-m --project=project_id --zone=europe-west1-b -- -4 -N -L 8080:cluster_name-m:8123</code>\n",
    "### Step 3: Open browser preview\n",
    "Within the cloud shell you'll find a button \"Preview on port 8080\" - this should open another browser window with the Jupyter connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 4.3 - GCP with PySpark and Jupyter Notebooks (Alternative 3: HdM JupyterHub)\n",
    "\n",
    "### Step 1: Create cluster\n",
    "See above\n",
    "### Step 2: establish a tunnel to our JupyterHub virtual machine\n",
    "You need to set up a tunnel in Putty: go to Connection->SSH->Tunnels and enter <code>10000</code> as \"source port\" and in the \"destination\" field <code>localhost:10000</code>. Hit \"Add\" and save the connection.\n",
    "<br><br>On a Mac you can use <code>ssh xxx@xxx -p 60022 -L 10000:localhost:10000</code>\n",
    "### Step 3: Open a browser using the tunnel\n",
    "See steps 3 and 4 above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages with pip\n",
    "Install pandas from a cell like this:<br>\n",
    "<code>import sys\n",
    "!{sys.executable} -m pip install pandas</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code of the Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df = sqlContext.read.load(\"gs://pyspark_example/order_items.csv\",\n",
    "                         format=\"com.databricks.spark.csv\",\n",
    "                         header=\"true\",\n",
    "                         inferSchema=\"true\")\n",
    "\n",
    "# how many distinct products\n",
    "df.select(\"product_id\").distinct().collect()\n",
    "\n",
    "# how many line items per product\n",
    "df.groupBy(\"product_id\").count().collect()\n",
    "\n",
    "# read the orders file\n",
    "df_orders = sqlContext.read.load(\"gs://pyspark_example/orders.csv\",\n",
    "                         format=\"com.databricks.spark.csv\",\n",
    "                         header=\"true\",\n",
    "                         inferSchema=\"true\")\n",
    "\n",
    "# show all possible order states\n",
    "df_orders.select(\"order_status\").distinct().collect()\n",
    "# calculate open order items per status and customer (without closer and cancelled orders)\n",
    "open_order_items = df.join(df_orders.filter((df_orders.order_status!='CLOSED') & (df_orders.order_status!='CANCELED')), \n",
    "                                     df.order_id == df_orders.order_id)\n",
    "\n",
    "open_order_items.groupBy(\"customer_id\", \"order_status\").count().collect()\n",
    "\n",
    "\n",
    "# write back results as a view (lambda architecture)\n",
    "result_df = open_order_items.groupBy(\"customer_id\", \"order_status\").count()\n",
    "result_df.write.save(\"gs://pyspark_example/results/\", format='csv')\n",
    "\n",
    "# show a histogram of line items per order\n",
    "pandas_df = df.groupBy(\"order_id\").count().withColumnRenamed('count', 'no_lines').groupBy('no_lines').count().toPandas()\n",
    "\n",
    "pandas_df.set_index('no_lines')\n",
    "\n",
    "pandas_df['count'].plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 4.4 - RapidMiner with Spark  & Hadoop\n",
    "Add this to the hosts file (Windows: C:\\Windows\\System32\\drivers\\etc\\hosts, Mac: /etc/hosts):<br>\n",
    "<code>xxx quickstart.cloudera</code>\n",
    "- NameNode Address: <code>quickstart.cloudera</code>\n",
    "- Advanced Parameters: <code>dfs.client.use.datanode.hostname</code> - set to true\n",
    "- Assembly Jar Location: <code>hdfs:///user/cloudera/spark-assembly.jar</code>\n",
    "- Hive Server Address: <code>quickstart.cloudera</code> (or xxx)\n",
    " - Username: xxx\n",
    " - Password: xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Lecture 5 - Data Extraction and Query Languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 5.5 - View Generation with sqoop\n",
    "1. <code>mysql -u xxx -p -D xxx\n",
    "CREATE TABLE order_statistics (order_customer_id BIGINT, order_status VARCHAR(200), sum_order_item_quantity BIGINT, count_order_id BIGINT);</code>\n",
    "2. <code>sqoop export --connect jdbc:mysql://localhost/retail_db --driver com.mysql.jdbc.Driver --username xxx --password xxx --table order_statistics --hcatalog-table order_statistics</code>\n",
    "3. <code>select * from order_statistics;</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 5.6 - View Generation in GCP\n",
    "Open the cloud shell and prepare the Police Departments Dataset (unzip and upload back to bucket) \n",
    "- <code>gsutil cp gs://sf_crime/Police_Department_Incidents.zip</code>\n",
    "- <code>unzip Police_Department_Incidents.zip</code>\n",
    "- <code>gsutil cp Police_Department_Incidents.csv gs://sf_crime/</code>\n",
    "- <code>rm ./Police_Department_Incidents</code>\n",
    "- <code>remove the zip file from the bucket</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Lecture 6 - Stream Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6.1 - Kafka producer / consumer\n",
    "SSH into our ubuntu machine \n",
    "- IP: xxx\n",
    "- Port: 60022\n",
    "- User: xxx\n",
    "- Password: provided in lecture\n",
    "\n",
    "Create a console producer\n",
    "1. Go to: <code>cd /opt/kafka/kafka_2.12-2.1.0/bin</code>\n",
    "2. Create a topic: <code>kafka-topics --zookeeper xxx:2181 --create --topic hdmm9 --partitions 1 --replication-factor 1</code>\n",
    "3. Create a “console producer”: <code>kafka-console-producer.sh --broker-list localhost:9092 --topic mytopic</code>\n",
    "\n",
    "Open another shell (see above)<br>\n",
    "Create a consumer: <code>kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic mytopic</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6.3 - GCP PubSub\n",
    "Also see the templates folder in JupyterHub (/notebooks/template/pubsub).\n",
    "\n",
    "1. Subscribe to the NY-Taxi stream: <code>gcloud pubsub subscriptions create test --topic projects/pubsub-public-data/topics/taxirides-realtime</code>\n",
    "2. Pull streaming data:<code>gcloud pubsub subscriptions pull projects/your project name/subscriptions/test</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6.3b - GCP PubSub with Python\n",
    "See the templates folder in JupyterHub (/notebooks/template/pubsub)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6.4 - Spark Streaming with Scala\n",
    "<code>import org.apache.spark.{SparkContext, SparkConf}\n",
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getRootLogger.setLevel(Level.WARN)\n",
    "val ssc = new StreamingContext(sc, Seconds(10))\n",
    "val lines = ssc.socketTextStream(\"localhost\", 30999, StorageLevel.MEMORY_AND_DISK_SER)\n",
    "val words = lines.flatMap(_.split(\" \"))\n",
    "val pairs = words.map((_, 1))\n",
    "val wordCounts = pairs.reduceByKey(_ + _)\n",
    "wordCounts.print()\n",
    "ssc.start()</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6.4b - Spark Streaming with PySpark (Shell)\n",
    "Enter this code in a pyspark shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from uuid import uuid1\n",
    "\n",
    "ssc = StreamingContext(sc, 10) # 10 second window\n",
    "\n",
    "kvs = KafkaUtils.createStream(ssc, \"xxx:2181\", \"raw-event-streaming-consumer\",{\"mytopic\":1})  # set my_topic to a Kafka topic to which you can publis\n",
    "\n",
    "lines = kvs.map(lambda x: x[1])\n",
    "counts = lines.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)\n",
    "counts.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6.4c - Spark Streaming with PySpark (Job-Execution)\n",
    "Enter this code in a file called <code>pyspark_streaming_kafka.py</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "if __name__ == \"__main__\":\n",
    "        sc = SparkContext(appName=\"PythonStreamingKafkaWordCount\")\n",
    "        ssc = StreamingContext(sc, 10) # 10 second window\n",
    "\n",
    "        kvs = KafkaUtils.createStream(ssc, \"xxx:2181\", \"raw-event-streaming-consumer\",{\"my_topic\":1})  # set my_topic to a Kafka topic to which you can publis\n",
    "\n",
    "        lines = kvs.map(lambda x: x[1])\n",
    "        counts = lines.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)\n",
    "        counts.pprint()\n",
    "\n",
    "        ssc.start()\n",
    "        ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit this file as a job to Spark:\n",
    "<code>spark-submit --master=yarn pyspark_streaming_kafka.py</code>\n",
    "\n",
    "Push some messages to the topic my_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6.4d - Spark Structured Streaming with PySpark (Spark v1.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def getSqlContextInstance(sparkContext):\n",
    "    \"\"\" Helper function to get the spark sql context (newer versions don't need this)\"\"\"\n",
    "    if ('sqlContextSingletonInstance' not in globals()):\n",
    "        globals()['sqlContextSingletonInstance'] = SQLContext(sparkContext)\n",
    "    return globals()['sqlContextSingletonInstance']\n",
    "\n",
    "def process(time, rdd):\n",
    "    \"\"\" process the rdd (of each window) based on structured streaming \"\"\"\n",
    "    print(\"========= %s =========\" % str(time)) # print the time of the window\n",
    "    try:\n",
    "        sqlContext = getSqlContextInstance(rdd.context)  # call helper function\n",
    "        rowRdd = rdd.map(lambda w: Row(word=w)) # create a Row object for each line of the windows input\n",
    "        wordsDataFrame = sqlContext.createDataFrame(rowRdd)  # create a DataFrame (-> StructuredStreaming)\n",
    "        wordsDataFrame.registerTempTable(\"words\") # register temp table\n",
    "        wordCountsDataFrame = sqlContext.sql(\"select word, count(*) as total from words group by word\") # count words\n",
    "        wordCountsDataFrame.show() # show the result\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "\n",
    "\n",
    "ssc = StreamingContext(sc, 5) # 5 second window\n",
    "kvs = KafkaUtils.createStream(ssc, \"141.62.117.222:2181\", \"raw-event-streaming-consumer\",{\"my_topic\":1}) # listen to kafka topic\n",
    "lines = kvs.map(lambda x: x[1]) # a kafka message is a tuple (key, value) - we want to analyze the value\n",
    "lines.foreachRDD(process) # we \"register\" the process method (see above) to be executed for each incoming rdd (i.e. per window)\n",
    "ssc.start()  # start structured streaming\n",
    "ssc.awaitTermination()  # wait for user termination (Ctrl-c) to stop stream processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Lecture 7 - NoSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 7.4 - MongoDB\n",
    "See the templates folder in JupyterHub (/notebooks/template/mongodb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
